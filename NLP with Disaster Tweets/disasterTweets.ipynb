{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading train and test data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess the text part of the data removing stopwords, tokenization and lemmatization\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "train['text'] = train['text'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 7613\n",
      "keyword 221\n",
      "location 3341\n",
      "target 2\n",
      "id [    1     4     5 ... 10871 10872 10873]\n",
      "keyword [nan 'ablaze' 'accident' 'aftershock' 'airplane%20accident' 'ambulance'\n",
      " 'annihilated' 'annihilation' 'apocalypse' 'armageddon' 'army' 'arson'\n",
      " 'arsonist' 'attack' 'attacked' 'avalanche' 'battle' 'bioterror'\n",
      " 'bioterrorism' 'blaze' 'blazing' 'bleeding' 'blew%20up' 'blight'\n",
      " 'blizzard' 'blood' 'bloody' 'blown%20up' 'body%20bag' 'body%20bagging'\n",
      " 'body%20bags' 'bomb' 'bombed' 'bombing' 'bridge%20collapse'\n",
      " 'buildings%20burning' 'buildings%20on%20fire' 'burned' 'burning'\n",
      " 'burning%20buildings' 'bush%20fires' 'casualties' 'casualty'\n",
      " 'catastrophe' 'catastrophic' 'chemical%20emergency' 'cliff%20fall'\n",
      " 'collapse' 'collapsed' 'collide' 'collided' 'collision' 'crash' 'crashed'\n",
      " 'crush' 'crushed' 'curfew' 'cyclone' 'damage' 'danger' 'dead' 'death'\n",
      " 'deaths' 'debris' 'deluge' 'deluged' 'demolish' 'demolished' 'demolition'\n",
      " 'derail' 'derailed' 'derailment' 'desolate' 'desolation' 'destroy'\n",
      " 'destroyed' 'destruction' 'detonate' 'detonation' 'devastated'\n",
      " 'devastation' 'disaster' 'displaced' 'drought' 'drown' 'drowned'\n",
      " 'drowning' 'dust%20storm' 'earthquake' 'electrocute' 'electrocuted'\n",
      " 'emergency' 'emergency%20plan' 'emergency%20services' 'engulfed'\n",
      " 'epicentre' 'evacuate' 'evacuated' 'evacuation' 'explode' 'exploded'\n",
      " 'explosion' 'eyewitness' 'famine' 'fatal' 'fatalities' 'fatality' 'fear'\n",
      " 'fire' 'fire%20truck' 'first%20responders' 'flames' 'flattened' 'flood'\n",
      " 'flooding' 'floods' 'forest%20fire' 'forest%20fires' 'hail' 'hailstorm'\n",
      " 'harm' 'hazard' 'hazardous' 'heat%20wave' 'hellfire' 'hijack' 'hijacker'\n",
      " 'hijacking' 'hostage' 'hostages' 'hurricane' 'injured' 'injuries'\n",
      " 'injury' 'inundated' 'inundation' 'landslide' 'lava' 'lightning'\n",
      " 'loud%20bang' 'mass%20murder' 'mass%20murderer' 'massacre' 'mayhem'\n",
      " 'meltdown' 'military' 'mudslide' 'natural%20disaster'\n",
      " 'nuclear%20disaster' 'nuclear%20reactor' 'obliterate' 'obliterated'\n",
      " 'obliteration' 'oil%20spill' 'outbreak' 'pandemonium' 'panic' 'panicking'\n",
      " 'police' 'quarantine' 'quarantined' 'radiation%20emergency' 'rainstorm'\n",
      " 'razed' 'refugees' 'rescue' 'rescued' 'rescuers' 'riot' 'rioting'\n",
      " 'rubble' 'ruin' 'sandstorm' 'screamed' 'screaming' 'screams' 'seismic'\n",
      " 'sinkhole' 'sinking' 'siren' 'sirens' 'smoke' 'snowstorm' 'storm'\n",
      " 'stretcher' 'structural%20failure' 'suicide%20bomb' 'suicide%20bomber'\n",
      " 'suicide%20bombing' 'sunk' 'survive' 'survived' 'survivors' 'terrorism'\n",
      " 'terrorist' 'threat' 'thunder' 'thunderstorm' 'tornado' 'tragedy'\n",
      " 'trapped' 'trauma' 'traumatised' 'trouble' 'tsunami' 'twister' 'typhoon'\n",
      " 'upheaval' 'violent%20storm' 'volcano' 'war%20zone' 'weapon' 'weapons'\n",
      " 'whirlwind' 'wild%20fires' 'wildfire' 'windstorm' 'wounded' 'wounds'\n",
      " 'wreck' 'wreckage' 'wrecked']\n",
      "location [nan 'Birmingham' 'Est. September 2012 - Bristol' ... 'Vancouver, Canada'\n",
      " 'London ' 'Lincoln']\n",
      "target [1 0]\n"
     ]
    }
   ],
   "source": [
    "# count unique values in each columns except text\n",
    "for col in train.columns:\n",
    "    if col != 'text':\n",
    "        print(col, train[col].nunique())\n",
    "        \n",
    "for col in train.columns:\n",
    "    if col != 'text':\n",
    "        print(col, train[col].unique())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    42\n",
       "0    19\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i want to check the distribution of the target where location is null and keyword is alsoo null\n",
    "\n",
    "train[train['keyword'].isnull()]['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan values in  location column and drop keyword where it is na\n",
    "\n",
    "train['location'] = train['location'].fillna('unknown')\n",
    "test['location'] = test['location'].fillna('unknown')\n",
    "train['keyword'] = train['keyword'].fillna(train['keyword'].mode()[0])\n",
    "test['keyword'] = test['keyword'].fillna(test['keyword'].mode()[0])\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop('target', axis=1)\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-fe2d7f89b0e4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text'] = X_train['keyword'] + ' ' + X_train['location'] + ' ' + X_train['text']\n",
      "<ipython-input-9-fe2d7f89b0e4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_val['text'] = X_val['keyword'] + ' ' + X_val['location'] + ' ' + X_val['text']\n"
     ]
    }
   ],
   "source": [
    "#combine all three columns to create a single text column\n",
    "#X_train['text'] = X_train['keyword'] + ' ' + X_train['location'] + ' ' + X_train['text']\n",
    "#X_val['text'] = X_val['keyword'] + ' ' + X_val['location'] + ' ' + X_val['text']\n",
    "#test['text'] = test['keyword'] + ' ' + test['location'] + ' ' + test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing keyword and location columns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_keyword = vectorizer.fit_transform(X_train['keyword'])\n",
    "X_val_keyword = vectorizer.transform(X_val['keyword'])\n",
    "test_keyword = vectorizer.transform(test['keyword'])\n",
    "\n",
    "X_train_location = vectorizer.fit_transform(X_train['location'])\n",
    "X_val_location = vectorizer.transform(X_val['location'])\n",
    "test_location = vectorizer.transform(test['location'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_text = vectorizer.fit_transform(X_train['text'])\n",
    "X_val_text = vectorizer.transform(X_val['text'])\n",
    "X_test_text = vectorizer.transform(test['text'])\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_text = tfidf.fit_transform(X_train['text'])\n",
    "X_val_text = tfidf.transform(X_val['text'])\n",
    "X_test_text = tfidf.transform(test['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Pipeline(steps=[('clf', MultinomialNB(alpha=0.1))])\n",
      "Best Parameters: {'clf': MultinomialNB(alpha=0.1), 'clf__alpha': 0.1}\n",
      "Best F1 Score: 0.7435283222858984\n",
      "Validation F1 Score: 0.7463884430176565\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assuming X_train_split, X_val, y_train_split, y_val are already defined\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "\t('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = [\n",
    "\t{\n",
    "\t\t'clf': [LogisticRegression()],\n",
    "\t\t'clf__C': [0.1, 1, 10]\n",
    "\t},\n",
    "\t{\n",
    "\t\t'clf': [RandomForestClassifier()],\n",
    "\t\t'clf__n_estimators': [50, 100, 200],\n",
    "\t\t'clf__max_depth': [None, 10, 20]\n",
    "\t},\n",
    "\t{\n",
    "\t\t'clf': [MultinomialNB()],\n",
    "\t\t'clf__alpha': [0.01, 0.1, 1]\n",
    "\t},\n",
    "\t{\n",
    "\t\t'clf': [XGBClassifier()],\n",
    "\t\t'clf__n_estimators': [50, 100, 200],\n",
    "\t\t'clf__max_depth': [3, 6, 9],\n",
    "\t\t'clf__learning_rate': [0.01, 0.1, 0.2]\n",
    "\t},\n",
    "\t{\n",
    "\t\t'clf': [SVC()],\n",
    "\t\t'clf__C': [0.1, 1, 10],\n",
    "\t\t'clf__kernel': ['linear', 'rbf']\n",
    "\t}\n",
    "]\n",
    "\n",
    "# Perform Grid Search CV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train_text, y_train)\n",
    "\n",
    "# Best model and parameters\n",
    "print(\"Best Model:\", grid_search.best_estimator_)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "val_predictions = best_model.predict(X_val_text)\n",
    "from sklearn.metrics import f1_score\n",
    "val_f1_score = f1_score(y_val, val_predictions)\n",
    "print(\"Validation F1 Score:\", val_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7463884430176565"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training a model with cross validation and f1 score as metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Training the model\n",
    "best_model.fit(X_train_text, y_train)\n",
    "y_pred = best_model.predict(X_val_text)\n",
    "f1_score(y_val, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on test data\n",
    "y_test = best_model.predict(X_test_text)\n",
    "\n",
    "#Saving the predictions in submission.csv with same format as sample sumission\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "submission['target'] = y_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
